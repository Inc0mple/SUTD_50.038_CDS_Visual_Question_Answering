{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 23:00:55.827703: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-15 23:00:56.657578: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-15 23:00:57.603144: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libnvinfer.so.8'; dlerror: libnvinfer.so.8: cannot open shared object file: No such file or directory\n",
      "2023-04-15 23:00:57.603210: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libnvinfer_plugin.so.8'; dlerror: libnvinfer_plugin.so.8: cannot open shared object file: No such file or directory\n",
      "2023-04-15 23:00:57.603213: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initialised!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import (\n",
    "    # Preprocessing / Common\n",
    "    AutoTokenizer, AutoFeatureExtractor,\n",
    "    # Text & Image Models (Now, image transformers like ViTModel, DeiTModel, BEiT can also be loaded using AutoModel)\n",
    "    AutoModel,\n",
    "    # Training / Evaluation\n",
    "    TrainingArguments, Trainer,\n",
    "    # Misc\n",
    "    logging\n",
    ")\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import os\n",
    "import json\n",
    "VQAV2_FILEPATH = \"./VQAv2 Annotations Preprocessed\"\n",
    "with open(f\"{VQAV2_FILEPATH}/VQAv2_answer_mapping.json\", \"r\") as json_file:\n",
    "    answer_to_id = json.load(json_file)\n",
    "id_to_answer = {v: k for k, v in answer_to_id.items()}\n",
    "answer_space = list(answer_to_id.keys())\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class MultimodalVQAModel_modified_GELU_noDropout(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_labels: int = len(answer_space),\n",
    "            intermediate_dim: int = 512,\n",
    "            pretrained_text_name: str = 'roberta-base',\n",
    "            pretrained_image_name: str = 'google/vit-base-patch16-224-in21k'):\n",
    "\n",
    "        super(MultimodalVQAModel_modified_GELU_noDropout, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.pretrained_text_name = pretrained_text_name\n",
    "        self.pretrained_image_name = pretrained_image_name\n",
    "\n",
    "        self.text_encoder = AutoModel.from_pretrained(\n",
    "            self.pretrained_text_name,\n",
    "        )\n",
    "        self.image_encoder = AutoModel.from_pretrained(\n",
    "            self.pretrained_image_name,\n",
    "        )\n",
    "\n",
    "        self.encoders_hidden_size = self.text_encoder.config.hidden_size + \\\n",
    "            self.image_encoder.config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoders_hidden_size,\n",
    "                      self.encoders_hidden_size*1),\n",
    "            nn.LayerNorm(self.encoders_hidden_size*1),\n",
    "            nn.GELU(),\n",
    "            # nn.Dropout(0.5),\n",
    "            nn.Linear(self.encoders_hidden_size*1, self.num_labels)\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: torch.LongTensor,\n",
    "            pixel_values: torch.FloatTensor,\n",
    "            attention_mask: Optional[torch.LongTensor] = None,\n",
    "            token_type_ids: Optional[torch.LongTensor] = None,\n",
    "            labels: Optional[torch.LongTensor] = None):\n",
    "\n",
    "        encoded_text = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        encoded_image = self.image_encoder(\n",
    "            pixel_values=pixel_values,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        logits = self.classifier(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    encoded_text['pooler_output'],\n",
    "                    encoded_image['pooler_output'],\n",
    "                ],\n",
    "                dim=1\n",
    "            )\n",
    "        )\n",
    "        out = {\n",
    "            \"logits\": logits\n",
    "        }\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(logits, labels)\n",
    "            out[\"loss\"] = loss\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def createMultimodalVQAModel(text='roberta-base', image='google/vit-base-patch16-224-in21k'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(text)\n",
    "    preprocessor = AutoFeatureExtractor.from_pretrained(image)\n",
    "\n",
    "\n",
    "    multi_model = MultimodalVQAModel_modified_GELU_noDropout(pretrained_text_name=text, pretrained_image_name=image).to(device)\n",
    "\n",
    "    return multi_model\n",
    "\n",
    "\n",
    "def get_step(checkpoint):\n",
    "    return int(checkpoint[11:])\n",
    "\n",
    "model_dir = 'Checkpoint_VQA_3129_Dropped_NaN'\n",
    "model_folder = 'roberta_base_g_vit_hidden_x1_GELU_NoDrop_yes_no_untouched'\n",
    "checkpoint_list = [cp for cp in os.listdir(os.path.join(\n",
    "    model_dir, model_folder)) if cp.find('checkpoint-') != -1]\n",
    "latest_cp = max(list(map(get_step, checkpoint_list)))\n",
    "trained_path = os.path.join(\n",
    "    model_dir, model_folder, f'checkpoint-{latest_cp}', 'pytorch_model.bin')\n",
    "model = createMultimodalVQAModel()\n",
    "model.load_state_dict(torch.load(trained_path))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"model initialised!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_text(texts: List[str]):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "    encoded_text = tokenizer(\n",
    "        text=texts,\n",
    "        padding='longest',\n",
    "        max_length=24,\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "        return_token_type_ids=True,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": encoded_text['input_ids'].squeeze(),\n",
    "        \"token_type_ids\": encoded_text['token_type_ids'].squeeze(),\n",
    "        \"attention_mask\": encoded_text['attention_mask'].squeeze(),\n",
    "    }\n",
    "\n",
    "\n",
    "def preprocess_images(image):\n",
    "    image = Image.fromarray(image[0])\n",
    "    preprocessor = AutoFeatureExtractor.from_pretrained(\n",
    "        \"microsoft/beit-base-patch16-224-pt22k-ft22k\")\n",
    "    processed_images = preprocessor(\n",
    "        images=[image.convert('RGB')],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return {\n",
    "        \"pixel_values\": processed_images['pixel_values'].squeeze(),\n",
    "    }\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def answer(input_image, input_question):\n",
    "    textDict = tokenize_text([input_question])\n",
    "\n",
    "    imgDict = preprocess_images([input_image])\n",
    "    \n",
    "    input_ids = textDict[\"input_ids\"].unsqueeze(0).to(device)\n",
    "    token_type_ids = textDict[\"token_type_ids\"].unsqueeze(0).to(device)\n",
    "    attention_mask = textDict[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "    pixel_values = imgDict[\"pixel_values\"].unsqueeze(0).to(device)\n",
    "    output = model(input_ids, pixel_values,\n",
    "                   attention_mask, token_type_ids)\n",
    "    preds = output[\"logits\"].argmax(axis=-1).cpu().numpy()[0]\n",
    "    return id_to_answer[preds]\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=answer,\n",
    "    inputs=[\"image\", \"text\"],\n",
    "    outputs=\"text\",\n",
    ")\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
